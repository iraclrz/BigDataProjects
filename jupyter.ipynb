from pyspark import SparkContext, SparkConf
import findspark
findspark.init()

# Create a SparkConf object
conf = SparkConf().setAppName("DataScience").setMaster("local[*]")

# Create a SparkContext
#sc.stop()                           # Use this line of code ONLY after first run of code !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
sc = SparkContext(conf=conf)

# Load the data as an RDD
rdd = sc.textFile("C:/books.csv")

# remove header from the rdd
no_header_rdd = rdd.filter(lambda row: row != header)

# Declare constant VARIABLES
header = rdd.first()